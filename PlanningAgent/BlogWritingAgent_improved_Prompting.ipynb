{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6bd2dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import operator\n",
    "from pathlib import Path\n",
    "from typing import TypedDict, List, Annotated, Literal\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.types import Send\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cc42339",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Task(BaseModel):\n",
    "    id: int\n",
    "    title: str\n",
    "\n",
    "    goal: str = Field(\n",
    "        ...,\n",
    "        description=\"One sentence describing what the reader should be able to do/understand after this section.\",\n",
    "    )\n",
    "    bullets: List[str] = Field(\n",
    "        ...,\n",
    "        min_length=3,\n",
    "        max_length=5,\n",
    "        description=\"3–5 concrete, non-overlapping subpoints to cover in this section.\",\n",
    "    )\n",
    "    target_words: int = Field(\n",
    "        ...,\n",
    "        description=\"Target word count for this section (120–450).\",\n",
    "    )\n",
    "    section_type: Literal[\n",
    "        \"intro\", \"core\", \"examples\", \"checklist\", \"common_mistakes\", \"conclusion\"\n",
    "    ] = Field(\n",
    "        ...,\n",
    "        description=\"Use 'common_mistakes' exactly once in the plan.\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e40e8607",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plan(BaseModel):\n",
    "    blog_title: str\n",
    "    audience: str = Field(..., description=\"Who this blog is for.\")\n",
    "    tone: str = Field(..., description=\"Writing tone (e.g., practical, crisp).\")\n",
    "    tasks: List[Task]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df6b53d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    topic: str\n",
    "    plan: Plan\n",
    "    sections: Annotated[List[str], operator.add]  # reducer concatenates worker outputs\n",
    "    final: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211eb578",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44d92db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def orchestrator(state: State) -> dict:\n",
    "    planner = llm.with_structured_output(Plan)\n",
    "\n",
    "    plan = planner.invoke(\n",
    "        [\n",
    "            SystemMessage(\n",
    "                content=(\n",
    "                    \"You are a senior technical writer and developer advocate. Your job is to produce a \"\n",
    "                    \"highly actionable outline for a technical blog post.\\n\\n\"\n",
    "                    \"Hard requirements:\\n\"\n",
    "                    \"- Create 5–7 sections (tasks) that fit a technical blog.\\n\"\n",
    "                    \"- Each section must include:\\n\"\n",
    "                    \"  1) goal (1 sentence: what the reader can do/understand after the section)\\n\"\n",
    "                    \"  2) 3–5 bullets that are concrete, specific, and non-overlapping\\n\"\n",
    "                    \"  3) target word count (120–450)\\n\"\n",
    "                    \"- Include EXACTLY ONE section with section_type='common_mistakes'.\\n\\n\"\n",
    "                    \"Make it technical (not generic):\\n\"\n",
    "                    \"- Assume the reader is a developer; use correct terminology.\\n\"\n",
    "                    \"- Prefer design/engineering structure: problem → intuition → approach → implementation → \"\n",
    "                    \"trade-offs → testing/observability → conclusion.\\n\"\n",
    "                    \"- Bullets must be actionable and testable (e.g., 'Show a minimal code snippet for X', \"\n",
    "                    \"'Explain why Y fails under Z condition', 'Add a checklist for production readiness').\\n\"\n",
    "                    \"- Explicitly include at least ONE of the following somewhere in the plan (as bullets):\\n\"\n",
    "                    \"  * a minimal working example (MWE) or code sketch\\n\"\n",
    "                    \"  * edge cases / failure modes\\n\"\n",
    "                    \"  * performance/cost considerations\\n\"\n",
    "                    \"  * security/privacy considerations (if relevant)\\n\"\n",
    "                    \"  * debugging tips / observability (logs, metrics, traces)\\n\"\n",
    "                    \"- Avoid vague bullets like 'Explain X' or 'Discuss Y'. Every bullet should state what \"\n",
    "                    \"to build/compare/measure/verify.\\n\\n\"\n",
    "                    \"Ordering guidance:\\n\"\n",
    "                    \"- Start with a crisp intro and problem framing.\\n\"\n",
    "                    \"- Build core concepts before advanced details.\\n\"\n",
    "                    \"- Include one section for common mistakes and how to avoid them.\\n\"\n",
    "                    \"- End with a practical summary/checklist and next steps.\\n\\n\"\n",
    "                    \"Output must strictly match the Plan schema.\"\n",
    "                )\n",
    "            ),\n",
    "            HumanMessage(content=f\"Topic: {state['topic']}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return {\"plan\": plan}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e06853d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fanout(state: State):\n",
    "    return [\n",
    "        Send(\n",
    "            \"worker\",\n",
    "            {\"task\": task, \"topic\": state[\"topic\"], \"plan\": state[\"plan\"]},\n",
    "        )\n",
    "        for task in state[\"plan\"].tasks\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5357935",
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(payload: dict) -> dict:\n",
    "\n",
    "    task = payload[\"task\"]\n",
    "    topic = payload[\"topic\"]\n",
    "    plan = payload[\"plan\"]\n",
    "\n",
    "    bullets_text = \"\\n- \" + \"\\n- \".join(task.bullets)\n",
    "\n",
    "    section_md = llm.invoke(\n",
    "        [\n",
    "            SystemMessage(\n",
    "    content=(\n",
    "        \"You are a senior technical writer and developer advocate. Write ONE section of a technical blog post in Markdown.\\n\\n\"\n",
    "        \"Hard constraints:\\n\"\n",
    "        \"- Follow the provided Goal and cover ALL Bullets in order (do not skip or merge bullets).\\n\"\n",
    "        \"- Stay close to the Target words (±15%).\\n\"\n",
    "        \"- Output ONLY the section content in Markdown (no blog title H1, no extra commentary).\\n\\n\"\n",
    "        \"Technical quality bar:\\n\"\n",
    "        \"- Be precise and implementation-oriented (developers should be able to apply it).\\n\"\n",
    "        \"- Prefer concrete details over abstractions: APIs, data structures, protocols, and exact terms.\\n\"\n",
    "        \"- When relevant, include at least one of:\\n\"\n",
    "        \"  * a small code snippet (minimal, correct, and idiomatic)\\n\"\n",
    "        \"  * a tiny example input/output\\n\"\n",
    "        \"  * a checklist of steps\\n\"\n",
    "        \"  * a diagram described in text (e.g., 'Flow: A -> B -> C')\\n\"\n",
    "        \"- Explain trade-offs briefly (performance, cost, complexity, reliability).\\n\"\n",
    "        \"- Call out edge cases / failure modes and what to do about them.\\n\"\n",
    "        \"- If you mention a best practice, add the 'why' in one sentence.\\n\\n\"\n",
    "        \"Markdown style:\\n\"\n",
    "        \"- Start with a '## <Section Title>' heading.\\n\"\n",
    "        \"- Use short paragraphs, bullet lists where helpful, and code fences for code.\\n\"\n",
    "        \"- Avoid fluff. Avoid marketing language.\\n\"\n",
    "        \"- If you include code, keep it focused on the bullet being addressed.\\n\"\n",
    "    )\n",
    ")\n",
    ",\n",
    "            HumanMessage(\n",
    "                content=(\n",
    "                    f\"Blog: {plan.blog_title}\\n\"\n",
    "                    f\"Audience: {plan.audience}\\n\"\n",
    "                    f\"Tone: {plan.tone}\\n\"\n",
    "                    f\"Topic: {topic}\\n\\n\"\n",
    "                    f\"Section: {task.title}\\n\"\n",
    "                    f\"Section type: {task.section_type}\\n\"\n",
    "                    f\"Goal: {task.goal}\\n\"\n",
    "                    f\"Target words: {task.target_words}\\n\"\n",
    "                    f\"Bullets:{bullets_text}\\n\"\n",
    "                )\n",
    "            ),\n",
    "        ]\n",
    "    ).content.strip()\n",
    "\n",
    "    return {\"sections\": [section_md]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd3d1283",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reducer(state: State) -> dict:\n",
    "\n",
    "    title = state[\"plan\"].blog_title\n",
    "    body = \"\\n\\n\".join(state[\"sections\"]).strip()\n",
    "    \n",
    "    final_md = f\"# {title}\\n\\n{body}\\n\"\n",
    "\n",
    "    # Save to file\n",
    "    filename = \"\".join(c if c.isalnum() or c in (\" \", \"_\", \"-\") else \"\" for c in title)\n",
    "    filename = filename.strip().lower().replace(\" \", \"_\") + \".md\"\n",
    "    Path(filename).write_text(final_md, encoding=\"utf-8\")\n",
    "\n",
    "    return {\"final\": final_md}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2a88acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x71e5c43d1510>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = StateGraph(State)\n",
    "g.add_node(\"orchestrator\", orchestrator)\n",
    "g.add_node(\"worker\", worker)\n",
    "g.add_node(\"reducer\", reducer)\n",
    "\n",
    "g.add_edge(START, \"orchestrator\")\n",
    "g.add_conditional_edges(\"orchestrator\", fanout, [\"worker\"])\n",
    "g.add_edge(\"worker\", \"reducer\")\n",
    "g.add_edge(\"reducer\", END)\n",
    "\n",
    "app = g.compile()\n",
    "\n",
    "app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca06a70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = app.invoke({\"topic\": \"Write a blog on Self Attention\", \"sections\": []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0cf21df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Demystifying Self-Attention: A Developer's Guide to Understanding and Implementing\n",
      "\n",
      "## Introduction to Self-Attention and Its Significance\n",
      "\n",
      "Self-attention is a neural network mechanism that allows a model to weigh and aggregate information from different positions within a single sequence. Unlike traditional attention mechanisms, which typically align elements of an input sequence to an external context (e.g., encoder-decoder attention in machine translation), self-attention operates **within** the same sequence, enabling each element to attend to every other element. This internal referencing allows the model to dynamically highlight relevant parts of the input when encoding a given token.\n",
      "\n",
      "The motivation for self-attention arises from limitations in classical sequence models like recurrent neural networks (RNNs) and convolutional neural networks (CNNs). RNNs process sequences step-by-step, which is inherently sequential and prevents efficient parallelization. They also struggle with capturing long-range dependencies due to vanishing gradients. CNNs offer parallelism but have limited receptive fields, requiring deep stacks to capture distant dependencies. Self-attention addresses these issues by **directly connecting all positions in a sequence**, allowing the model to learn relationships regardless of distance.\n",
      "\n",
      "Self-attention is a core component of the Transformer architecture, where it replaces recurrence and convolutions for sequence modeling. In a Transformer, each input token is transformed into queries, keys, and values; self-attention computes attention scores between queries and keys to create weighted sums of values. This forms an attention layer within each Transformer block, which stacks multiple layers to build powerful representations for tasks like translation, summarization, and language modeling.\n",
      "\n",
      "Key advantages of self-attention include:\n",
      "\n",
      "- **Parallelization:** Unlike RNNs, self-attention computations over tokens can be done simultaneously, leveraging modern hardware efficiently.\n",
      "- **Long-range dependency capture:** Every element can attend to every other element directly, enabling global context awareness.\n",
      "- **Adaptability:** Self-attention dynamically focuses on relevant positions for each token, which aids in handling diverse sequences and tasks.\n",
      "\n",
      "A useful analogy to grasp self-attention is to think of reading a sentence with sticky notes attached to every word. When understanding a specific word’s meaning, you look at all other words’ sticky notes to see how they relate and adjust your focus accordingly. Similarly, self-attention assigns \"importance scores\" that help the model decide which words in a sequence are most relevant when processing a target word.\n",
      "\n",
      "In summary, self-attention shifts sequence modeling from linear and fixed-context approaches to a flexible, fully interconnected mechanism that underpins state-of-the-art deep learning models today.\n",
      "\n",
      "## Core Concepts and Mathematical Formulation of Self-Attention\n",
      "\n",
      "Self-attention operates on an input sequence by transforming it into three matrices: Query (Q), Key (K), and Value (V). Suppose the input sequence is represented as a matrix \\(X \\in \\mathbb{R}^{T \\times d}\\), where \\(T\\) is the sequence length and \\(d\\) is the embedding dimension. We apply three learned linear projections:\n",
      "\n",
      "\\[\n",
      "Q = XW_Q, \\quad K = XW_K, \\quad V = XW_V\n",
      "\\]\n",
      "\n",
      "where \\(W_Q, W_K, W_V \\in \\mathbb{R}^{d \\times d_k}\\) are weight matrices, and \\(d_k\\) is typically less than or equal to \\(d\\). The resulting shapes are:\n",
      "\n",
      "- \\(Q \\in \\mathbb{R}^{T \\times d_k}\\)  \n",
      "- \\(K \\in \\mathbb{R}^{T \\times d_k}\\)  \n",
      "- \\(V \\in \\mathbb{R}^{T \\times d_v}\\), often \\(d_v = d_k\\).\n",
      "\n",
      "---\n",
      "\n",
      "### Computing Attention Scores\n",
      "\n",
      "The core step is to compute compatibility scores between queries and keys. For each position \\(i\\) in the sequence, the attention scores with all positions \\(j\\) are given by the scaled dot-product:\n",
      "\n",
      "\\[\n",
      "\\text{scores}_{i,j} = \\frac{Q_i \\cdot K_j^\\top}{\\sqrt{d_k}}\n",
      "\\]\n",
      "\n",
      "Concretely, this is a matrix multiplication:\n",
      "\n",
      "\\[\n",
      "\\text{scores} = \\frac{Q K^\\top}{\\sqrt{d_k}}\n",
      "\\]\n",
      "\n",
      "Scaling by \\(\\sqrt{d_k}\\) prevents the dot-products from growing too large in magnitude, which can saturate the softmax and degrade gradients.\n",
      "\n",
      "---\n",
      "\n",
      "### Softmax and Numerical Stability\n",
      "\n",
      "The raw scores are converted into attention weights via the softmax function for each query position \\(i\\):\n",
      "\n",
      "\\[\n",
      "\\alpha_{i,j} = \\frac{\\exp(\\text{scores}_{i,j})}{\\sum_{k=1}^T \\exp(\\text{scores}_{i,k})}\n",
      "\\]\n",
      "\n",
      "To improve numerical stability when implementing the softmax, subtract the maximum score in each row before exponentiation:\n",
      "\n",
      "```python\n",
      "# Pseudocode for stable softmax row-wise\n",
      "max_score = np.max(scores, axis=1, keepdims=True)\n",
      "exp_scores = np.exp(scores - max_score)\n",
      "attention_weights = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
      "```\n",
      "\n",
      "This subtraction avoids large exponentials which can cause overflow.\n",
      "\n",
      "---\n",
      "\n",
      "### Generating the Output\n",
      "\n",
      "The output for each position is the weighted sum of the value vectors, using attention weights as coefficients:\n",
      "\n",
      "\\[\n",
      "\\text{output}_i = \\sum_{j=1}^T \\alpha_{i,j} V_j\n",
      "\\]\n",
      "\n",
      "In matrix form:\n",
      "\n",
      "\\[\n",
      "\\text{output} = \\text{attention_weights} \\times V\n",
      "\\]\n",
      "\n",
      "This produces an output matrix \\(\\mathbb{R}^{T \\times d_v}\\), where each row summarizes relevant information from the sequence for that position.\n",
      "\n",
      "---\n",
      "\n",
      "### Minimal Working Example: Scaled Dot-Product Self-Attention\n",
      "\n",
      "Below is a minimal example in NumPy demonstrating self-attention on a small dummy input:\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "\n",
      "# Dummy input: sequence length T=3, embedding dim d=4\n",
      "X = np.array([\n",
      "    [1, 0, 1, 0],\n",
      "    [0, 2, 0, 1],\n",
      "    [1, 1, 1, 1]\n",
      "], dtype=np.float32)\n",
      "\n",
      "# Random weight matrices (for simplicity, using identity matrices)\n",
      "d_k = d_v = 4\n",
      "W_Q = np.eye(4)\n",
      "W_K = np.eye(4)\n",
      "W_V = np.eye(4)\n",
      "\n",
      "Q = X @ W_Q    # shape (3,4)\n",
      "K = X @ W_K    # shape (3,4)\n",
      "V = X @ W_V    # shape (3,4)\n",
      "\n",
      "# Compute scaled dot-product attention scores\n",
      "scores = (Q @ K.T) / np.sqrt(d_k)\n",
      "\n",
      "# Stable softmax\n",
      "max_scores = np.max(scores, axis=1, keepdims=True)\n",
      "exp_scores = np.exp(scores - max_scores)\n",
      "attention_weights = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
      "\n",
      "# Compute output\n",
      "output = attention_weights @ V\n",
      "\n",
      "print(\"Attention Weights:\\n\", attention_weights)\n",
      "print(\"Output:\\n\", output)\n",
      "```\n",
      "\n",
      "**Output Explanation:**  \n",
      "- `attention_weights` shows how much each position attends to every other position.  \n",
      "- `output` is the sequence representation after self-attention, mixing information from all positions based on attention weights.\n",
      "\n",
      "---\n",
      "\n",
      "### Summary\n",
      "\n",
      "Self-attention transforms an input sequence into queries, keys, and values, computes scaled dot-products between queries and keys to get scores, normalizes these via softmax (with max-subtraction for stability), and finally creates outputs by weighted summation over values. This mechanism allows each position to dynamically attend to relevant tokens, enabling powerful contextual representations.\n",
      "\n",
      "## Implementing Multi-Head Self-Attention in Practice\n",
      "\n",
      "Multi-head self-attention extends the basic self-attention mechanism by running multiple attention operations (heads) in parallel. Intuitively, each head can focus on different parts or patterns within the input sequence simultaneously. This diversity allows the model to capture richer relationships and features compared to a single attention head, ultimately improving model capacity and expressiveness without increasing the embedding size per head.\n",
      "\n",
      "### Projection and Concatenation of Multiple Heads\n",
      "\n",
      "Given an input tensor \\( X \\in \\mathbb{R}^{B \\times T \\times D} \\) (batch size \\( B \\), sequence length \\( T \\), embedding dimension \\( D \\)), multi-head attention first linearly projects \\( X \\) into queries, keys, and values for each head. If there are \\( H \\) heads, each head has a dimensionality \\( d = D/H \\).\n",
      "\n",
      "For each head \\( h \\):\n",
      "- \\( Q_h = X W_h^Q \\in \\mathbb{R}^{B \\times T \\times d} \\)\n",
      "- \\( K_h = X W_h^K \\in \\mathbb{R}^{B \\times T \\times d} \\)\n",
      "- \\( V_h = X W_h^V \\in \\mathbb{R}^{B \\times T \\times d} \\)\n",
      "\n",
      "These projections create multiple sets of queries, keys, and values corresponding to different learned subspaces.\n",
      "\n",
      "After computing scaled dot-product attention for each head, the results are concatenated along the last dimension to form \\( \\mathbb{R}^{B \\times T \\times D} \\) and passed through a final linear transformation \\( W^O \\) to mix information from all heads.\n",
      "\n",
      "### PyTorch Code Sketch\n",
      "\n",
      "```python\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "\n",
      "class MultiHeadSelfAttention(nn.Module):\n",
      "    def __init__(self, embed_dim, num_heads):\n",
      "        super().__init__()\n",
      "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
      "        self.embed_dim = embed_dim\n",
      "        self.num_heads = num_heads\n",
      "        self.head_dim = embed_dim // num_heads\n",
      "        \n",
      "        self.qkv_proj = nn.Linear(embed_dim, embed_dim * 3)  # Project X to Q,K,V together\n",
      "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
      "    \n",
      "    def forward(self, x, mask=None):\n",
      "        B, T, D = x.size()\n",
      "        qkv = self.qkv_proj(x)  # [B, T, 3*D]\n",
      "        qkv = qkv.reshape(B, T, 3, self.num_heads, self.head_dim)  # [B, T, 3, H, d]\n",
      "        q, k, v = qkv.unbind(dim=2)  # Each [B, T, H, d]\n",
      "        \n",
      "        # Transpose for attention: [B, H, T, d]\n",
      "        q, k, v = [t.transpose(1, 2) for t in (q, k, v)]\n",
      "        \n",
      "        # Scaled dot-product attention\n",
      "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)  # [B, H, T, T]\n",
      "        if mask is not None:\n",
      "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
      "        attn = F.softmax(scores, dim=-1)\n",
      "        \n",
      "        out = torch.matmul(attn, v)  # [B, H, T, d]\n",
      "        out = out.transpose(1, 2).contiguous().reshape(B, T, D)  # [B, T, D]\n",
      "        return self.out_proj(out)\n",
      "```\n",
      "\n",
      "### Performance Considerations\n",
      "\n",
      "- **Memory and compute cost:** Multi-head attention requires maintaining \\( 3 \\times H \\times B \\times T \\times d \\) floats for Q, K, and V, plus intermediate scores of size \\( B \\times H \\times T \\times T \\). Since the attention score matrix scales quadratically with sequence length \\( T \\), memory and compute cost can become a bottleneck for very long sequences.\n",
      "- **Number of heads:** Increasing heads \\( H \\) linearly increases parallel computations but may reduce per-head dimension \\( d \\), influencing capacity and granularity of learned representations.\n",
      "- **Batch size:** Larger batch sizes improve GPU utilization but must be balanced against available memory.\n",
      "\n",
      "### Common Optimizations\n",
      "\n",
      "- **Masking:** Attention masks disable computation for padding tokens or future tokens (in causal attention). Implemented by setting scores corresponding to masked positions to \\(-\\infty\\) before softmax to avoid attention on them.\n",
      "- **Efficient batch processing:** Use batched matrix multiplications and contiguous memory operations to leverage GPU acceleration fully.\n",
      "- **Memory reuse:** Reuse buffers when possible and avoid unnecessary tensor copies (e.g., using `.contiguous()` only when needed).\n",
      "- **Sparse or approximate attention:** For very long sequences, consider sparse attention variants that reduce the \\( T^2 \\) scaling. This is a trade-off between accuracy and efficiency.\n",
      "\n",
      "By carefully projecting inputs, paralleling multiple attention heads, and optimizing memory and computation, multi-head self-attention effectively balances model capacity with practical performance constraints.\n",
      "\n",
      "## Common Mistakes When Working with Self-Attention and How to Avoid Them\n",
      "\n",
      "### Incorrect Scaling Factor in Dot-Product Attention\n",
      "\n",
      "The scaled dot-product attention uses a scale factor \\( \\frac{1}{\\sqrt{d_k}} \\) (where \\( d_k \\) is the dimensionality of the key vectors) to prevent extremely large values in the dot product, which can cause gradient instability. Omitting or miscalculating this scale causes exponential growth in attention logits, leading to saturating softmax outputs and vanishing or exploding gradients.\n",
      "\n",
      "**Best practice:** Always scale the dot product by \\(\\frac{1}{\\sqrt{d_k}}\\), e.g.:\n",
      "\n",
      "```python\n",
      "scale = key.size(-1) ** 0.5\n",
      "scores = torch.matmul(query, key.transpose(-2, -1)) / scale\n",
      "```\n",
      "\n",
      "This normalizes the variance of the dot products and stabilizes training.\n",
      "\n",
      "---\n",
      "\n",
      "### Misuse of Attention Masking Leading to Future Token Leakage\n",
      "\n",
      "In autoregressive models (e.g., GPT), attention masking prevents each token from attending to future tokens. Incorrectly implemented masks can leak future information, breaking the causal property.\n",
      "\n",
      "**Failing example:**\n",
      "\n",
      "```python\n",
      "# Incorrect mask with zeros where it should mask, allowing future tokens' attention\n",
      "mask = torch.tril(torch.ones(seq_len, seq_len))  # Should be 1 or 0 as a mask\n",
      "scores = scores.masked_fill(mask == 0, float('-inf'))\n",
      "```\n",
      "\n",
      "If the mask logic is inverted, future tokens will contribute to attention.\n",
      "\n",
      "**Fix:** Use a causal mask such that positions \\(j > i\\) are masked:\n",
      "\n",
      "```python\n",
      "mask = torch.tril(torch.ones(seq_len, seq_len)).bool()\n",
      "scores = scores.masked_fill(~mask, float('-inf'))\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### Shape Mismatch Errors in Q, K, V Projection Layers\n",
      "\n",
      "Common shape-related bugs arise from incorrect tensor dimensions during linear projections of Q, K, V, especially when batching and multi-head splitting are involved.\n",
      "\n",
      "- Q, K, V input shapes are typically `(batch_size, seq_len, embed_dim)`\n",
      "- After linear projection, shape should remain `(batch_size, seq_len, embed_dim)`\n",
      "- Splitting into heads reshapes to `(batch_size, num_heads, seq_len, head_dim)`\n",
      "\n",
      "**Debugging Tips:**\n",
      "\n",
      "- Use `print` or logging to output tensor shapes after every projection and reshape.\n",
      "- Add asserts to verify:\n",
      "\n",
      "```python\n",
      "assert query.shape == (batch_size, num_heads, seq_len, head_dim), \"Query shape mismatch\"\n",
      "assert key.shape == query.shape, \"Key shape mismatch with query\"\n",
      "assert value.shape == query.shape, \"Value shape mismatch\"\n",
      "```\n",
      "\n",
      "Shape mismatches often cause runtime errors or silent bugs in attention scores.\n",
      "\n",
      "---\n",
      "\n",
      "### Ignoring Numerical Stability in Softmax Computation\n",
      "\n",
      "Naively applying softmax on large logits can cause exponent overflow, resulting in `NaN`s.\n",
      "\n",
      "**Problematic code:**\n",
      "\n",
      "```python\n",
      "weights = torch.softmax(scores, dim=-1)  # scores may have large positive values\n",
      "```\n",
      "\n",
      "**Mitigation:** Subtract the max logit from `scores` along the softmax dimension to stabilize:\n",
      "\n",
      "```python\n",
      "scores = scores - scores.max(dim=-1, keepdim=True).values\n",
      "weights = torch.softmax(scores, dim=-1)\n",
      "```\n",
      "\n",
      "This shift does not change the softmax output but prevents exponent overflow.\n",
      "\n",
      "---\n",
      "\n",
      "### Neglecting Padding and Length Masking in Variable-Length Batches\n",
      "\n",
      "When processing batches with sequences of varying lengths, padded tokens should be masked to prevent attention to or from padded positions.\n",
      "\n",
      "**Debugging approach:**\n",
      "\n",
      "- Create a padding mask with shape `(batch_size, seq_len)`\n",
      "- Verify masking by inspecting attention weights or sum of masked positions:\n",
      "\n",
      "```python\n",
      "# Example padding_mask: True for valid tokens, False for padding\n",
      "attention_weights = attention_weights.masked_fill(~padding_mask.unsqueeze(1).unsqueeze(2), 0)\n",
      "```\n",
      "\n",
      "- Track downstream outputs to ensure padding tokens contribute no information.\n",
      "\n",
      "Neglecting padding masks leads to spurious attention scores skewing model outputs and degraded performance.\n",
      "\n",
      "---\n",
      "\n",
      "By addressing these common pitfalls—correct scaling, masking future tokens properly, ensuring shape correctness, maintaining numerical stability, and handling padding—developers can implement robust self-attention modules suitable for production-grade models.\n",
      "\n",
      "## Observability and Debugging Tips for Self-Attention Modules\n",
      "\n",
      "When developing self-attention layers, systematic observability is key to catching silent failures and understanding model behavior.\n",
      "\n",
      "- **Logging Strategies**  \n",
      "  Log shapes of Queries (Q), Keys (K), and Values (V) tensors after their computation. This ensures dimensions align (e.g., `[batch, seq_len, head_dim]`). Record statistics of attention weight distributions after softmax—mean, stddev, min, max—to detect anomalies such as uniform or extremely peaked weights. Track summary stats rather than raw tensors to minimize overhead.\n",
      "\n",
      "- **Sanity Checks**  \n",
      "  - Verify that attention weights sum to 1 across keys for each query vector:  \n",
      "    ```python\n",
      "    # attention_weights shape: [batch, heads, query_len, key_len]\n",
      "    sums = attention_weights.sum(dim=-1)\n",
      "    assert torch.allclose(sums, torch.ones_like(sums), atol=1e-5), \"Attention weights must sum to 1\"\n",
      "    ```\n",
      "  - Detect NaNs or infinities in Q, K, V, attention scores, and weights, e.g.:  \n",
      "    ```python\n",
      "    def check_finite(tensor, name):\n",
      "        if not torch.isfinite(tensor).all():\n",
      "            raise ValueError(f\"Non-finite values detected in {name}\")\n",
      "    check_finite(Q, \"Q\")\n",
      "    check_finite(attention_weights, \"attention_weights\")\n",
      "    ```\n",
      "  These checks catch numerical instability early, helping avoid silent model degradation.\n",
      "\n",
      "- **Visualizing Attention Maps**  \n",
      "  Use heatmaps to visualize averaged attention weights per head for a batch of input sequences. For example, after extracting attention weights of shape `[batch, heads, seq_len, seq_len]`, average over the batch dimension and plot per head:  \n",
      "  ```python\n",
      "  import matplotlib.pyplot as plt\n",
      "  import seaborn as sns\n",
      "  \n",
      "  avg_attention = attention_weights.mean(dim=0)  # [heads, seq_len, seq_len]\n",
      "  head = 0\n",
      "  sns.heatmap(avg_attention[head].cpu().detach().numpy())\n",
      "  plt.title(f\"Attention Map - Head {head}\")\n",
      "  plt.xlabel(\"Key Position\")\n",
      "  plt.ylabel(\"Query Position\")\n",
      "  plt.show()\n",
      "  ```\n",
      "  Patterns such as diagonal dominance imply strong positional focus; diffuse patterns suggest global context usage.\n",
      "\n",
      "- **Profiling Throughput and Memory**  \n",
      "  Attach hooks at self-attention layers to measure forward pass time per batch and peak memory use. Example using PyTorch hooks:  \n",
      "  ```python\n",
      "  import time\n",
      "  \n",
      "  def timing_hook(module, input, output):\n",
      "      start = time.time()\n",
      "      # forward already computed, so consider measuring externally or use Profiler\n",
      "      elapsed = time.time() - start\n",
      "      print(f\"Self-attention layer {module} forward duration: {elapsed:.4f}s\")\n",
      "  \n",
      "  for module in model.modules():\n",
      "      if isinstance(module, SelfAttentionModule):\n",
      "          module.register_forward_hook(timing_hook)\n",
      "  ```\n",
      "  Use torch.cuda.max_memory_allocated() before and after forward pass for GPU memory. These metrics inform bottlenecks and scaling impact.\n",
      "\n",
      "- **Unit Testing Best Practices**  \n",
      "  Write unit tests asserting:  \n",
      "  - Output shape matches expectations for various input sizes.  \n",
      "  - Attention weights sum constraint holds strictly.  \n",
      "  - Handling of edge cases such as zero vectors or constant inputs does not produce NaNs or degenerate outputs.  \n",
      "  - Consistency of softmax outputs (e.g., non-negative, sums to 1).  \n",
      "  Testing with controlled inputs boosts confidence in correctness before integration into larger models.\n",
      "\n",
      "Together, these strategies help create robust, interpretable self-attention implementations that are easier to debug, maintain, and optimize.\n",
      "\n",
      "## Summary, Production Checklist and Next Steps\n",
      "\n",
      "### Key Points Recap\n",
      "- **Self-attention architecture** computes context-aware token representations by relating each token to all others via query, key, and value projections.\n",
      "- Implementation requires careful tensor dimension management, efficient batched matrix multiplications, and masking to ignore padding or future tokens in autoregressive setups.\n",
      "- Common pitfalls include numerical instability in softmax leading to overflow/underflow, incorrect mask application causing leakage of future information, and inefficient reshaping that increases latency.\n",
      "\n",
      "### Production Readiness Checklist\n",
      "- [ ] **Correctness**: Verify shapes and dimension ordering for Q, K, V tensors; confirm mask shapes align with attention scores.\n",
      "- [ ] **Numerical Stability**: Implement scaled dot-product attention with softmax input scaled by √(d_k) and apply `torch.nn.functional.softmax` or equivalent; clip logits if needed.\n",
      "- [ ] **Masking**: Use additive masks (large negative values) to block attention to padding or future tokens; test mask effects on output thoroughly.\n",
      "- [ ] **Efficiency**: Batch computations and use fused kernels/libraries (e.g., PyTorch’s `nn.MultiheadAttention`).\n",
      "- [ ] **Observability**: Log intermediate attention weights; add unit tests for attention outputs with known inputs; monitor latency and memory usage.\n",
      "  \n",
      "### Suggested Enhancements\n",
      "- Implement **relative position encodings** to better capture token position relationships beyond absolute indices.\n",
      "- Explore **memory-efficient variants** like Linformer or Performer to reduce quadratic complexity while maintaining accuracy.\n",
      "- Consider **sparse attention** mechanisms for long sequences to optimize cost.\n",
      "\n",
      "### Recommended Resources\n",
      "- PyTorch’s official [`nn.MultiheadAttention`](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html) for practical, battle-tested code.\n",
      "- Hugging Face Transformers library with numerous pretrained self-attention-based models and well-documented implementations.\n",
      "- Open-source projects like [Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html) for pedagogical code.\n",
      "\n",
      "### Next Steps\n",
      "- Integrate self-attention layers into a small NLP or vision project from scratch to solidify understanding.\n",
      "- Fine-tune existing transformer models on custom datasets to observe attention behavior and tuning impact.\n",
      "- Experiment with modifications such as custom masks or position encodings to deepen practical skills and discover performance trade-offs. \n",
      "\n",
      "Following this checklist ensures a robust, efficient, and maintainable self-attention implementation ready for production deployment.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(out[\"final\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbbe89a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d723ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Planning Agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
